## ğŸ›¡ï¸ Guardrails in OpenAI SDK 

Guardrails in OpenAI SDK are safety rules or checkers.

- **Input guardrails** check the userâ€™s question first. If itâ€™s safe, the agent continues.
- **Output guardrails** then check the agentâ€™s reply. If something unsafe was missed, it will stop the agent from answering.
- These guardrails act like a safety block around the AI to stop anything unsafe or harmful.

â¡ï¸ Guardrails make sure both the question and the answer are safe **before anything is shown to the user**.

### âœ… Summary:
Guardrails work like filters.  
They watch both the question and the answer to make sure nothing unsafe is allowed.  
If anything looks harmful or restricted, the agent stops automatically.